{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096914f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Context Retrieval\n",
    "# Code for DDXPlus only\n",
    "# All classification context (DDXPlus, SymptomsDisease, Symptom2Disease) already retrieved into context.csv file\n",
    "# DO NOT RUN CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd8efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Env setup\n",
    "import getpass\n",
    "import os\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LLM\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "#os.environ[\"OPENAI_BASE_URL\"] = os.getenv('OPENAI_BASE_URL')\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Neo4j env (lmkg)\n",
    "url = \"neo4j://43.140.200.9:7687\"\n",
    "username =\"neo4j\"\n",
    "password = \"20230408\"\n",
    "graph = Neo4jGraph(\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password,\n",
    "    refresh_schema=False\n",
    ")\n",
    "\n",
    "# CSV env\n",
    "import sys\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from pathlib import Path\n",
    "from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(r'C:\\Users\\Sin Yee\\Desktop\\rag_techniques')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc547b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query & Knowledge Agent Graph\n",
    "## Helper utilities\n",
    "from typing import List, Optional, Literal, TypedDict, Union\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.types import Command\n",
    "from langchain_core.messages import HumanMessage, trim_messages\n",
    "\n",
    "# Query agent\n",
    "class QState(MessagesState):\n",
    "    next: str\n",
    "    planner_executed: bool = False\n",
    "\n",
    "# Knowledge agent\n",
    "class State(MessagesState):\n",
    "    next: str\n",
    "    original_question: str = \"\"\n",
    "    references: dict = {}\n",
    "    completed_agents: list = []\n",
    "    excluded_agents: list = []\n",
    "\n",
    "def make_supervisor_node(llm: BaseChatModel, members: list[str], agent_scopes: dict = None) -> callable:\n",
    "    \"\"\"\n",
    "    Create supervisor node that only routes between agents for reference retrieval\n",
    "    \"\"\"\n",
    "    if agent_scopes is None:\n",
    "        agent_scopes = {\n",
    "            \"lmkg\": \"General medical queries using knowledge graph: diseases, exams, indicators, symptoms, complications etc.\",\n",
    "            \"hkg\": \"For questions on symptom.\",\n",
    "            \"ds\": \"For questions on symptoms or treatments.\",\n",
    "            \"primekg\": \"To retrieve drugs indicated for diseases.\",\n",
    "            \"drugreviews\": \"To retrieve drugs along with patient reviews.\",\n",
    "            \"wiki\": \"For disease definitions and overviews.\",\n",
    "            \"mayoclinic\": \"For clinical info: causes, treatments, symptoms, complications.\",\n",
    "            \"llmself\": \"Fallback only if others are irrelevant.\"\n",
    "        }\n",
    "    \n",
    "    def supervisor_node(state: State) -> Command:\n",
    "        \"\"\"Routes to agents for reference retrieval only.\"\"\"\n",
    "        # Get the original question\n",
    "        if not state.get(\"original_question\"):\n",
    "            original_question = next(\n",
    "                msg.content for msg in state[\"messages\"] \n",
    "                if msg.type == \"human\"\n",
    "            )\n",
    "        else:\n",
    "            original_question = state[\"original_question\"]\n",
    "        \n",
    "        completed_agents = state.get(\"completed_agents\", [])\n",
    "        references = state.get(\"references\", {})\n",
    "        excluded_agents = state.get(\"excluded_agents\", [])\n",
    "\n",
    "        # Fallback REPLACEMENT logic\n",
    "        if \"llmself\" not in completed_agents and \"llmself\" in members:\n",
    "            for agent in completed_agents:\n",
    "                ref = references.get(agent, \"\")\n",
    "                # Check for both: No information or empty cypher retrieved_result\n",
    "                is_no_info = ref == \"No information retrieved\"\n",
    "                is_empty_cypher = (\n",
    "                    isinstance(ref, dict)\n",
    "                    and \"generated_cypher\" in ref  # identify as cypher_reference\n",
    "                    and (\n",
    "                        not ref.get(\"retrieved_result\")\n",
    "                        or (isinstance(ref.get(\"retrieved_result\"), list) and not any(str(r).strip() for r in ref[\"retrieved_result\"]))\n",
    "                    )\n",
    "                )\n",
    "                if is_no_info or is_empty_cypher:\n",
    "                    print(f\"⚠️ Replacing low-quality agent '{agent}' with fallback agent 'llmself'.\")\n",
    "                    \n",
    "                    # Remove low-quality agent\n",
    "                    completed_agents.remove(agent)\n",
    "                    references.pop(agent, None)\n",
    "                    excluded_agents.append(agent)  # Mark as excluded\n",
    "                    \n",
    "                    return Command(goto=\"llmself\", update={\n",
    "                        \"next\": \"llmself\",\n",
    "                        \"original_question\": original_question,\n",
    "                        \"completed_agents\": completed_agents,\n",
    "                        \"excluded_agents\": excluded_agents\n",
    "                    })\n",
    "\n",
    "        # Check if we have references from all agents - if so, finish.\n",
    "        if len(completed_agents) >= 6:\n",
    "            return Command(goto=END, update={\n",
    "                \"next\": \"FINISH\",\n",
    "                \"original_question\": original_question\n",
    "            })\n",
    "        \n",
    "        available_agents = [agent for agent in members]\n",
    "        remaining_agents = [agent for agent in available_agents \n",
    "                            if agent not in completed_agents and agent not in excluded_agents]\n",
    "        \n",
    "        # Check if all agents have been completed\n",
    "        if not remaining_agents:\n",
    "            return Command(goto=END, update={\n",
    "                \"next\": \"FINISH\",\n",
    "                \"original_question\": original_question\n",
    "            })\n",
    "        \n",
    "        # Create agent scope descriptions for remaining agents\n",
    "        remaining_agent_scopes = {agent: agent_scopes.get(agent, \"General purpose agent\") \n",
    "                                for agent in remaining_agents}\n",
    "        \n",
    "        # Create options with only remaining agents\n",
    "        options = [\"FINISH\"] + remaining_agents\n",
    "        \n",
    "        # System prompt for agent selection\n",
    "        system_prompt_updated = (\n",
    "            f\"You are a supervisor selecting agents for medical reference retrieval. \"\n",
    "            f\"Choose one agent at a time based on query relevance.\\n\\n\"\n",
    "            f\"Agent options and specialties:\\n\"\n",
    "        )\n",
    "        \n",
    "        for agent, scope in remaining_agent_scopes.items():\n",
    "            system_prompt_updated += f\"- {agent}: {scope}\\n\"\n",
    "        \n",
    "        system_prompt_updated += (\n",
    "            f\"\\nCompleted agents: {completed_agents} (Target: 6 agents)\\n\"\n",
    "            f\"Available agents: {remaining_agents}\\n\\n\"\n",
    "            f\"GOAL: Select up to 6 MOST RELEVANT agents.\\n\"\n",
    "            f\"Progress: {len(completed_agents)}/6 done\\n\\n\"\n",
    "            f\"Instructions:\\n\"\n",
    "            f\"1. If 6 agents are done, select FINISH\\n\"\n",
    "            f\"2. Choose the most relevant agent from remaining\\n\"\n",
    "            f\"3. Prioritize by question type:\\n\"\n",
    "            f\"   - Symptoms: hkg > ds > mayoclinic > wiki > lmkg > drugreviews\\n\"\n",
    "            f\"   - Drugs/Medications: drugreviews > primekg \\n\"\n",
    "            f\"   - Complications: lmkg > mayoclinic\\n\"\n",
    "            f\"   - Fallback (only if others unsuitable): llmself\\n\"\n",
    "            f\"4. Pick the next best from: {remaining_agents}\\n\\n\"\n",
    "            f\"DO NOT select any agents not listed in 'Valid responses' below.\\n\\n\"\n",
    "            f\"Valid responses: {options}\\n\"\n",
    "            f\"Use 6 best agents, then FINISH.\"\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt_updated},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {original_question}\\n\\nSelect the most relevant agent:\"}\n",
    "        ]\n",
    "        \n",
    "        class Router(TypedDict):\n",
    "            \"\"\"Worker to route to next. Select the most relevant agent for reference retrieval.\"\"\"\n",
    "            next: str\n",
    "            \n",
    "        response = llm.with_structured_output(Router).invoke(messages)\n",
    "        goto = response[\"next\"]\n",
    "        \n",
    "        print(f\"Question: {original_question}\")\n",
    "        print(f\"Remaining agents: {remaining_agents}\")\n",
    "        print(f\"LLM selected: {goto}\")\n",
    "        print(f\"Valid options: {options}\")\n",
    "        \n",
    "        # Validate that the response is in our options\n",
    "        if goto not in options:\n",
    "            print(f\"Warning: Invalid selection '{goto}'. Defaulting to FINISH.\")\n",
    "            goto = \"FINISH\"\n",
    "            \n",
    "        if goto == \"FINISH\":\n",
    "            goto = END\n",
    "            \n",
    "        return Command(goto=goto, update={\n",
    "            \"next\": goto, \n",
    "            \"original_question\": original_question\n",
    "        })\n",
    "    \n",
    "    return supervisor_node\n",
    "\n",
    "## Tools (remain the same for retrieval)\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def planner_tool():\n",
    "    planner_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a query planning agent for a medical knowledge system. \n",
    "        Determine if the user's question is:\n",
    "        \n",
    "        1. \"single-step\" - Direct factual questions\n",
    "           e.g., \"What is diabetes?\", \"What are symptoms of flu?\", \"Which drug treats malaria?\"\n",
    "        \n",
    "        2. \"multi-step\" - Complex questions needing multiple facts or reasoning\n",
    "           e.g., \"Compare treatment options for diabetes vs hypertension\", \n",
    "           \"What's the best treatment for hypertension in a diabetic patient with renal impairment?\"\n",
    "        \n",
    "        Respond with either \"single-step\" or \"multi-step\".\"\"\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    planner_chain = LLMChain(llm=llm, prompt=planner_prompt)\n",
    "\n",
    "    def planner_agent(question: str):\n",
    "        return planner_chain.invoke({\"input\": question})['text']\n",
    "    return planner_agent\n",
    "\n",
    "## Tools for reference retrieval only\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA # vector\n",
    "from langchain.chains import GraphCypherQAChain # cypher\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate \n",
    "\n",
    "def lmkg_retrieval_tool():\n",
    "    # A) Tool's helper function\n",
    "    def build_schema(graph):\n",
    "        node_info = graph.query(\"\"\"\n",
    "            MATCH (n)\n",
    "            WITH labels(n) AS label_list, keys(n) AS props\n",
    "            UNWIND label_list AS label\n",
    "            RETURN DISTINCT label, collect(DISTINCT props) AS property_keys\n",
    "        \"\"\")\n",
    "        \n",
    "        rel_info = graph.query(\"\"\"\n",
    "            MATCH ()-[r]->()\n",
    "            RETURN DISTINCT type(r) AS rel_type, keys(r) AS property_keys\n",
    "        \"\"\")\n",
    "\n",
    "        # Build string\n",
    "        schema = \"Node properties are the following:\\n\"\n",
    "        for row in node_info:\n",
    "            label = row['label']\n",
    "            props = ', '.join(row['property_keys'][0]) if row['property_keys'] else ''\n",
    "            schema += f\"{label} {{{props}}}\\n\"\n",
    "\n",
    "        schema += \"\\nThe relationships are the following:\\n\"\n",
    "        for row in rel_info:\n",
    "            rel_type = row['rel_type']\n",
    "            schema += f\"(:X)-[:{rel_type}]->(:Y)\\n\"\n",
    "\n",
    "        return schema\n",
    "\n",
    "    # B) Vector search\n",
    "    lmkg_vector = FAISS.load_local(\n",
    "        \"lmkg_faiss\",\n",
    "        OpenAIEmbeddings(model='text-embedding-3-small'),\n",
    "        allow_dangerous_deserialization=True)\n",
    "\n",
    "    from typing import Union, Dict, Any\n",
    "    \n",
    "    def vector_search(input_data: Union[str, Dict[str, Any]]) -> dict:\n",
    "        \"\"\"Returns retrieved documents and final answer.\"\"\"\n",
    "        question = input_data if isinstance(input_data, str) else input_data.get(\"input\", \"\")\n",
    "        if not question:\n",
    "            return {\"error\": \"No question provided\"}\n",
    "\n",
    "        # Retrieve documents\n",
    "        retrieved_docs = lmkg_vector.similarity_search(question, k=4)\n",
    "        retrieved_context = [doc.page_content for doc in retrieved_docs]\n",
    "\n",
    "        return {\n",
    "            \"query\": question,\n",
    "            \"retrieved_context\": retrieved_context\n",
    "        }\n",
    "\n",
    "\n",
    "    # C) Cypher search\n",
    "    CYPHER_GENERATION_TEMPLATE = \"\"\"\n",
    "    Task: Generate precise Cypher query to answer the question:\n",
    "    {question}\n",
    "\n",
    "    Requirements:\n",
    "    1. MUST start from node ID: {node_id} using WHERE id(n) = {node_id}\n",
    "    2. MUST wrap labels/relationships with spaces in backticks.  \n",
    "    E.g., [:`Active Ingredient`], [:Complication]\n",
    "    3. Use only ONE relationship type\n",
    "    4. Return ONLY what's needed to answer the question\n",
    "    5. Use this schema:\n",
    "\n",
    "    Schema:\n",
    "    {schema}\n",
    "\n",
    "    Return ONLY the executable Cypher query with no additional text.\n",
    "    \"\"\"\n",
    "\n",
    "    CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"schema\", \"question\", \"node_id\"], template=CYPHER_GENERATION_TEMPLATE\n",
    "    )\n",
    "\n",
    "    cypher_chain = GraphCypherQAChain.from_llm(\n",
    "        cypher_llm = llm,\n",
    "        qa_llm = llm, graph=graph, verbose=True,\n",
    "        cypher_prompt=CYPHER_GENERATION_PROMPT,\n",
    "        return_direct=True, # bypass qa_llm\n",
    "        return_intermediate_steps=True, # return cypher query & retrieved context\n",
    "        allow_dangerous_requests=True\n",
    "    )\n",
    "\n",
    "    # insert graph_schema\n",
    "    cypher_chain.graph_schema = build_schema(graph)\n",
    "    schema = build_schema(graph)\n",
    "\n",
    "    # D) LMKG Agent\n",
    "    # Use vector search & cypher query to explore KG\n",
    "    from langchain.agents import Tool, AgentExecutor, create_openai_functions_agent\n",
    "    from langchain import hub\n",
    "    from typing import Dict, Any, Union\n",
    "\n",
    "    # Wrap cypher chain in a function with proper input handling\n",
    "    def cypher_search(input_data: Union[str, Dict[str, Any]]) -> dict:\n",
    "        \"\"\"Executes Cypher and returns query, raw result, and LLM answer.\"\"\"\n",
    "        try:\n",
    "            question = input_data if isinstance(input_data, str) else input_data.get(\"input\", \"\")\n",
    "            if not question:\n",
    "                return {\"error\": \"No question provided\"}\n",
    "\n",
    "            similar_nodes = lmkg_vector.similarity_search(question, k=1)\n",
    "            if not similar_nodes:\n",
    "                return {\"error\": \"No matching nodes found\"}\n",
    "\n",
    "            node_id = similar_nodes[0].metadata['node_id']\n",
    "            print(f\"Retrieved node id: {node_id}\")\n",
    "\n",
    "            # Inject into GraphCypherQAChain\n",
    "            cypher_response = cypher_chain({\n",
    "                \"query\": question,\n",
    "                \"schema\": schema,\n",
    "                \"node_id\": node_id\n",
    "            })\n",
    "\n",
    "            print(\"cypher_response:\")\n",
    "            print(cypher_response)\n",
    "\n",
    "            intermediate = cypher_response.get(\"intermediate_steps\", [])\n",
    "            \n",
    "            generated_cypher = next((step[\"query\"] for step in intermediate if \"query\" in step), None)\n",
    "            retrieved_context = cypher_response.get(\"result\", [])\n",
    "\n",
    "            return {\n",
    "                \"query\": question,\n",
    "                \"node_id\": node_id,\n",
    "                \"generated_cypher\": generated_cypher,\n",
    "                \"retrieved_result\": retrieved_context\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    tools = [\n",
    "        Tool(\n",
    "            name=\"med_vector_qa\",\n",
    "            func=vector_search,\n",
    "            description=\"\"\"Use for open-ended, semantic, or fuzzy medical questions requiring\n",
    "            contextual understanding from vector-based retrieval.\n",
    "            Examples:\n",
    "            \"What is ADHD?\"\n",
    "            \"Explain the mechanism of diabetes.\"\n",
    "            \"Long-term effects of COVID-19?\"\n",
    "            Only use this tool once.\n",
    "            \"\"\",\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"med_cypher_qa\",\n",
    "            func=cypher_search,\n",
    "            description=\"\"\"\"Use for specific, factual medical queries (about diseases, symptoms, complications, \n",
    "            affected organs, treatments, drugs etc.) using structured graph data. \n",
    "            Examples:\n",
    "            \"What is the symptom of ADHF?\"\n",
    "            \"Organs affected by HIV?\"\n",
    "            \"Complications of diabetes?\"\n",
    "            \"Drug for tuberculosis?\"\n",
    "            Only use this tool once.\n",
    "            \"\"\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "    agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "\n",
    "    # Create agent executor\n",
    "    lmkg_agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, return_intermediate_steps=True)\n",
    "    return lmkg_agent_executor, lmkg_vector\n",
    "\n",
    "def hkg_retrieval_tool():\n",
    "    \"\"\"HKG tool for reference retrieval only\"\"\"\n",
    "    hkg_vector = FAISS.load_local(\n",
    "        \"faiss_hkg\",\n",
    "        OpenAIEmbeddings(model='text-embedding-ada-002'),\n",
    "        allow_dangerous_deserialization=True)\n",
    "    return hkg_vector\n",
    "\n",
    "def ds_retrieval_tool():\n",
    "    ds_vector = FAISS.load_local(\n",
    "        \"diseases_symptoms_faiss\",\n",
    "        OpenAIEmbeddings(model='text-embedding-ada-002'),\n",
    "        allow_dangerous_deserialization=True)\n",
    "    return ds_vector\n",
    "\n",
    "def primekg_retrieval_tool():\n",
    "    \"\"\"PrimeKG tool for reference retrieval only\"\"\"\n",
    "    primekg_vector = FAISS.load_local(\n",
    "        \"primekg_faiss\",\n",
    "        OpenAIEmbeddings(model='text-embedding-ada-002'),\n",
    "        allow_dangerous_deserialization=True)\n",
    "    return primekg_vector\n",
    "\n",
    "def drugreviews_retrieval_tool():\n",
    "    drugreviews_vector = FAISS.load_local(\n",
    "        \"drug_reviews_faiss\",\n",
    "        OpenAIEmbeddings(model='text-embedding-ada-002'),\n",
    "        allow_dangerous_deserialization=True)\n",
    "    return drugreviews_vector\n",
    "\n",
    "def wiki_retrieval_tool():\n",
    "    \"\"\"Wiki tool retrieval\"\"\"\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI(api_key=os.environ[\"DEEPSEEK_API_KEY\"], base_url=os.environ[\"DEEPSEEK_BASE_URL\"])\n",
    "\n",
    "    def extract_entity_from_query(query):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"What is the main entity or concept in this query: '{query}'? Extract only the core term (e.g., 'hyponatremia', not 'causes of hyponatremia'). Return only the entity name in one line.\"}]\n",
    "        )\n",
    "        entity = response.choices[0].message.content.strip()\n",
    "        print(f'Extracted entity: {entity}')\n",
    "        return entity.replace(\" \", \"_\")\n",
    "    \n",
    "    return extract_entity_from_query\n",
    "\n",
    "def mayoclinic_retrieval_tool():\n",
    "    \"\"\"MayoClinic tool retrieval\"\"\"\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI(api_key=os.environ[\"DEEPSEEK_API_KEY\"], base_url=os.environ[\"DEEPSEEK_BASE_URL\"])\n",
    "\n",
    "    def extract_entity_from_query(query):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"What is the main entity or concept in this query: '{query}'? Extract only the core term (e.g., 'hyponatremia', not 'causes of hyponatremia'). Return only the entity name in one line.\"}]\n",
    "        )\n",
    "        entity = response.choices[0].message.content.strip()\n",
    "        print(f'Extracted entity: {entity}')\n",
    "        return entity\n",
    "    \n",
    "    return extract_entity_from_query\n",
    "\n",
    "def llmself_tool():\n",
    "    knowledge_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a knowledgeable AI. Answer concisely and precisely using only your internal knowledge. No external context.\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    knowledge_chain = LLMChain(llm=llm, prompt=knowledge_prompt)\n",
    "\n",
    "    def knowledge_agent(question: str):\n",
    "        \"\"\"Agent that answers using only its internal knowledge\"\"\"\n",
    "        return knowledge_chain.invoke({\"input\": question})['text']\n",
    "    return knowledge_agent\n",
    "\n",
    "## Worker nodes (Query Agent)\n",
    "def planner_node(state: QState):\n",
    "    question = next(\n",
    "        (msg.content for msg in reversed(state[\"messages\"]) \n",
    "         if msg.type == \"human\" and not hasattr(msg, 'name')), \n",
    "        None\n",
    "    )\n",
    "    \n",
    "    if question is None:\n",
    "        question = next(\n",
    "            (msg.content for msg in state[\"messages\"] if msg.type == \"human\"),\n",
    "            \"No question found\"\n",
    "        )\n",
    "    \n",
    "    planner_agent_executor = planner_tool()\n",
    "    result = planner_agent_executor(question)\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=result, name=\"planner\")\n",
    "        ],\n",
    "        \"planner_executed\": True\n",
    "    }\n",
    "\n",
    "def multi_step_node(state: QState):\n",
    "    original_question = None\n",
    "    for msg in state[\"messages\"]:\n",
    "        if msg.type == \"human\" and not hasattr(msg, 'name'):\n",
    "            original_question = msg.content\n",
    "            break\n",
    "    \n",
    "    if not original_question:\n",
    "        for msg in state[\"messages\"]:\n",
    "            if msg.type == \"human\":\n",
    "                original_question = msg.content\n",
    "                break\n",
    "    \n",
    "    if not original_question:\n",
    "        original_question = \"No question found\"\n",
    "    \n",
    "    subquery_prompt = f\"\"\"\n",
    "    Original query: {original_question}\n",
    "\n",
    "    \\nBreak the medical query into exactly two sub-queries. \n",
    "    Each sub-query should: \n",
    "    Be SIMPLE and SPECIFIC; \n",
    "    Focus on a different, narrow aspect of the original query;\n",
    "    Avoid long or reasoning-based formulations. \n",
    "    \n",
    "    \\nFormat your response as: \n",
    "    1. [sub-query 1] \n",
    "    2. [sub-query 2]\n",
    "    \"\"\"\n",
    "    \n",
    "    result = llm.invoke(subquery_prompt)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=result.content, name=\"subquery\")\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def query_agent_supervisor_node(state: QState):\n",
    "    return {\"messages\": state[\"messages\"]}\n",
    "\n",
    "## Worker nodes (Knowledge Agent) - Modified for reference retrieval only\n",
    "def lmkg_node(state: State):\n",
    "    question = state.get(\"original_question\") or next(\n",
    "        msg.content for msg in state[\"messages\"] \n",
    "        if msg.type == \"human\"\n",
    "    )\n",
    "    lmkg_agent_executor, lmkg_retriever = lmkg_retrieval_tool()\n",
    "    response = lmkg_agent_executor.invoke({\"input\": question})\n",
    "    result = response['output']\n",
    "\n",
    "    # Get intermediate_steps from response\n",
    "    intermediate_steps = response.get(\"intermediate_steps\", [])\n",
    "    \n",
    "    def extract_reference_from_steps(intermediate_steps):\n",
    "        \"\"\"Extract reference information from agent steps, prioritizing non-empty results\"\"\"\n",
    "        vector_reference = None\n",
    "        cypher_reference = None\n",
    "        cypher_has_results = False\n",
    "        \n",
    "        for step in intermediate_steps:\n",
    "            if isinstance(step, tuple) and len(step) >= 2:\n",
    "                tool_name = step[0].tool if hasattr(step[0], 'tool') else str(step[0])\n",
    "                tool_result = step[1]\n",
    "                \n",
    "                if isinstance(tool_result, dict):\n",
    "                    # Handle vector_qa tool\n",
    "                    if 'med_vector_qa' in tool_name or 'vector' in tool_name.lower():\n",
    "                        retrieved_context = tool_result.get(\"retrieved_context\", [])\n",
    "                        if retrieved_context and any(ctx.strip() for ctx in retrieved_context):\n",
    "                            vector_reference = retrieved_context\n",
    "                    \n",
    "                    # Handle cypher_qa tool\n",
    "                    if 'med_cypher_qa' in tool_name or 'cypher' in tool_name.lower():\n",
    "                        generated_cypher = tool_result.get(\"generated_cypher\", \"\")\n",
    "                        retrieved_result = tool_result.get(\"retrieved_result\", \"\")\n",
    "                        \n",
    "                        # Check if cypher has meaningful results\n",
    "                        if retrieved_result and retrieved_result != [] and any(str(item).strip() for item in (retrieved_result if isinstance(retrieved_result, list) else [retrieved_result])):\n",
    "                            cypher_has_results = True\n",
    "                        \n",
    "                        # Store cypher information regardless of whether result is empty\n",
    "                        cypher_reference = {\n",
    "                            \"generated_cypher\": generated_cypher or \"N/A\",\n",
    "                            \"retrieved_result\": retrieved_result\n",
    "                        }\n",
    "\n",
    "        \n",
    "        # Priority logic:\n",
    "        # 1. If both tools used and cypher has results → return cypher reference\n",
    "        # 2. If both tools used but cypher has no results → return vector reference\n",
    "        # 3. If only cypher used → return cypher reference (even if empty)\n",
    "        # 4. If only vector used → return vector reference\n",
    "        # 5. If neither used → return \"No information retrieved\"\n",
    "        \n",
    "        if cypher_reference and vector_reference:  # Both tools were used\n",
    "            if cypher_has_results:\n",
    "                return cypher_reference  # Cypher has results, use it\n",
    "            else:\n",
    "                return vector_reference  # Cypher empty, use vector\n",
    "        elif cypher_reference:  # Only cypher was used\n",
    "            return cypher_reference\n",
    "        elif vector_reference:  # Only vector was used\n",
    "            return vector_reference\n",
    "        else:  # No tools were used\n",
    "            return \"No information retrieved\"\n",
    "    \n",
    "    reference = extract_reference_from_steps(intermediate_steps)\n",
    "\n",
    "    # Update references and mark as completed    \n",
    "    references = state.get(\"references\", {})\n",
    "    references[\"lmkg\"] = reference\n",
    "\n",
    "    completed_agents = state.get(\"completed_agents\", [])\n",
    "    if \"lmkg\" not in completed_agents:\n",
    "        completed_agents.append(\"lmkg\")\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=f\"LMKG reference retrieved\", name=\"lmkg\")\n",
    "        ],\n",
    "        \"completed_agents\": completed_agents,\n",
    "        \"references\": references  # Fixed: was \"reference\" instead of \"references\"\n",
    "    }\n",
    "\n",
    "def hkg_node(state: State):\n",
    "    question = state.get(\"original_question\") or next(\n",
    "        msg.content for msg in state[\"messages\"] \n",
    "        if msg.type == \"human\"\n",
    "    )\n",
    "    \n",
    "    # Only retrieve reference, no LLM processing\n",
    "    hkg_retriever = hkg_retrieval_tool()\n",
    "    response = hkg_retriever.similarity_search(question, k=1)\n",
    "    reference = response[0].page_content if response else \"No reference found\"\n",
    "    \n",
    "    references = state.get(\"references\", {})\n",
    "    references[\"hkg\"] = reference\n",
    "\n",
    "    completed_agents = state.get(\"completed_agents\", [])\n",
    "    if \"hkg\" not in completed_agents:\n",
    "        completed_agents.append(\"hkg\")\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=f\"HKG reference retrieved\", name=\"hkg\")\n",
    "        ],\n",
    "        \"completed_agents\": completed_agents,\n",
    "        \"references\": references\n",
    "    }\n",
    "\n",
    "def ds_node(state: State):\n",
    "    question = state.get(\"original_question\") or next(\n",
    "        msg.content for msg in state[\"messages\"] \n",
    "        if msg.type == \"human\"\n",
    "    )\n",
    "    \n",
    "    # Only retrieve reference, no LLM processing\n",
    "    ds_retriever = ds_retrieval_tool()\n",
    "    response = ds_retriever.similarity_search(question, k=1)\n",
    "    reference = response[0].page_content if response else \"No reference found\"\n",
    "    \n",
    "    references = state.get(\"references\", {})\n",
    "    references[\"ds\"] = reference\n",
    "\n",
    "    completed_agents = state.get(\"completed_agents\", [])\n",
    "    if \"ds\" not in completed_agents:\n",
    "        completed_agents.append(\"ds\")\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=f\"DS reference retrieved\", name=\"ds\")\n",
    "        ],\n",
    "        \"completed_agents\": completed_agents,\n",
    "        \"references\": references\n",
    "    }\n",
    "\n",
    "def primekg_node(state: State):\n",
    "    question = state.get(\"original_question\") or next(\n",
    "        msg.content for msg in state[\"messages\"] \n",
    "        if msg.type == \"human\"\n",
    "    )\n",
    "    \n",
    "    # Only retrieve reference, no LLM processing\n",
    "    primekg_retriever = primekg_retrieval_tool()\n",
    "    response = primekg_retriever.similarity_search(question, k=1)\n",
    "    reference = response[0].page_content if response else \"No reference found\"\n",
    "    \n",
    "    references = state.get(\"references\", {})\n",
    "    references[\"primekg\"] = reference\n",
    "\n",
    "    completed_agents = state.get(\"completed_agents\", [])\n",
    "    if \"primekg\" not in completed_agents:\n",
    "        completed_agents.append(\"primekg\")\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=f\"PrimeKG reference retrieved\", name=\"primekg\")\n",
    "        ],\n",
    "        \"completed_agents\": completed_agents,\n",
    "        \"references\": references\n",
    "    }\n",
    "\n",
    "def drugreviews_node(state: State):\n",
    "    question = state.get(\"original_question\") or next(\n",
    "        msg.content for msg in state[\"messages\"] \n",
    "        if msg.type == \"human\"\n",
    "    )\n",
    "    \n",
    "    # Only retrieve reference, no LLM processing\n",
    "    drugreviews_retriever = drugreviews_retrieval_tool()\n",
    "    response = drugreviews_retriever.similarity_search(question, k=1)\n",
    "    reference = response[0].page_content if response else \"No reference found\"\n",
    "    \n",
    "    references = state.get(\"references\", {})\n",
    "    references[\"drugreviews\"] = reference\n",
    "\n",
    "    completed_agents = state.get(\"completed_agents\", [])\n",
    "    if \"drugreviews\" not in completed_agents:\n",
    "        completed_agents.append(\"drugreviews\")\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=f\"drugreviews reference retrieved\", name=\"drugreviews\")\n",
    "        ],\n",
    "        \"completed_agents\": completed_agents,\n",
    "        \"references\": references\n",
    "    }\n",
    "\n",
    "def wiki_node(state: State):\n",
    "    from wiki_crawler import crawl_wikipedia_entity\n",
    "    question = state.get(\"original_question\") or next(\n",
    "        msg.content for msg in state[\"messages\"] \n",
    "        if msg.type == \"human\"\n",
    "    )\n",
    "    \n",
    "    # Wiki data retrieval\n",
    "    query_entity = wiki_retrieval_tool()\n",
    "    entity = query_entity(question)\n",
    "    wiki_text = crawl_wikipedia_entity(entity, articles_limit=1)\n",
    "\n",
    "    # Store as reference\n",
    "    reference = wiki_text[:1500]\n",
    "    references = state.get(\"references\", {})\n",
    "    references[\"wiki\"] = reference\n",
    "\n",
    "    completed_agents = state.get(\"completed_agents\", [])\n",
    "    if \"wiki\" not in completed_agents:\n",
    "        completed_agents.append(\"wiki\")\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=f\"Wiki reference retrieved\", name=\"wiki\")\n",
    "        ],\n",
    "        \"completed_agents\": completed_agents,\n",
    "        \"references\": references\n",
    "    }\n",
    "\n",
    "def mayoclinic_node(state: State):\n",
    "    from mayoclinic_symptom_crawler import crawl_mayoclinic_entity\n",
    "    question = state.get(\"original_question\") or next(\n",
    "        msg.content for msg in state[\"messages\"] \n",
    "        if msg.type == \"human\"\n",
    "    )\n",
    "    \n",
    "    # MayoClinic data retrieval\n",
    "    query_entity = mayoclinic_retrieval_tool()\n",
    "    entity = query_entity(question)\n",
    "    mayoclinic_text = crawl_mayoclinic_entity(entity)\n",
    "\n",
    "    # Store as reference\n",
    "    reference = mayoclinic_text\n",
    "    references = state.get(\"references\", {})\n",
    "    references[\"mayoclinic\"] = reference\n",
    "\n",
    "    completed_agents = state.get(\"completed_agents\", [])\n",
    "    if \"mayoclinic\" not in completed_agents:\n",
    "        completed_agents.append(\"mayoclinic\")\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=f\"Mayo Clinic reference retrieved\", name=\"mayoclinic\")\n",
    "        ],\n",
    "        \"completed_agents\": completed_agents,\n",
    "        \"references\": references\n",
    "    }\n",
    "\n",
    "def llmself_node(state: State):\n",
    "    question = state.get(\"original_question\") or next(\n",
    "        msg.content for msg in state[\"messages\"] \n",
    "        if msg.type == \"human\"\n",
    "    )\n",
    "    \n",
    "    # Generate LLM answer using internal knowledge\n",
    "    llmself_agent_executor = llmself_tool()\n",
    "    result = llmself_agent_executor(question)\n",
    "    \n",
    "    # Store the LLM-generated answer directly as the reference\n",
    "    references = state.get(\"references\", {})\n",
    "    references[\"llmself\"] = result  # Store the actual answer as reference\n",
    "\n",
    "    completed_agents = state.get(\"completed_agents\", [])\n",
    "    if \"llmself\" not in completed_agents:\n",
    "        completed_agents.append(\"llmself\")\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=f\"LLM Self Agent completed\", name=\"llmself\")\n",
    "        ],\n",
    "        \"completed_agents\": completed_agents,\n",
    "        \"references\": references\n",
    "    }\n",
    "\n",
    "# Create supervisor and verifier nodes\n",
    "knowledge_agent_supervisor_node = make_supervisor_node(\n",
    "    llm, \n",
    "    [\"lmkg\", \"hkg\", \"ds\", \"primekg\", \"drugreviews\", \"wiki\", \"mayoclinic\", \"llmself\"],\n",
    "    agent_scopes={\n",
    "        \"lmkg\": \"General medical queries using knowledge graph: diseases, exams, indicators, symptoms, complications etc.\",\n",
    "        \"hkg\": \"For questions on symptom.\",\n",
    "        \"ds\": \"For questions on symptoms or treatments.\",\n",
    "        \"primekg\": \"To retrieve drugs indicated for diseases.\",\n",
    "        \"drugreviews\": \"To retrieve drugs along with patient reviews.\",\n",
    "        \"wiki\": \"For disease definitions and overviews.\",\n",
    "        \"mayoclinic\": \"For clinical info: causes, treatments, symptoms, complications.\",\n",
    "        \"llmself\": \"Fallback only if others are irrelevant.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create graph (Query Agent) - Modified to bypass supervisor\n",
    "# Modified planner_node to handle single-step directly\n",
    "def planner_node(state: QState):\n",
    "    question = next(\n",
    "        (msg.content for msg in reversed(state[\"messages\"]) \n",
    "         if msg.type == \"human\" and not hasattr(msg, 'name')), \n",
    "        None\n",
    "    )\n",
    "    \n",
    "    if question is None:\n",
    "        question = next(\n",
    "            (msg.content for msg in state[\"messages\"] if msg.type == \"human\"),\n",
    "            \"No question found\"\n",
    "        )\n",
    "    \n",
    "    planner_agent_executor = planner_tool()\n",
    "    result = planner_agent_executor(question)\n",
    "    \n",
    "    # Check if it's single-step and preserve the original question\n",
    "    if \"single-step\" in result.lower():\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result, name=\"planner\"),\n",
    "                HumanMessage(content=question, name=\"single_step\")  # Preserve original question\n",
    "            ],\n",
    "            \"planner_executed\": True\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result, name=\"planner\")\n",
    "            ],\n",
    "            \"planner_executed\": True\n",
    "        }\n",
    "\n",
    "query_builder = StateGraph(QState)\n",
    "query_builder.add_node(\"planner\", planner_node)\n",
    "query_builder.add_node(\"multi_step\", multi_step_node)\n",
    "\n",
    "# Start directly with planner\n",
    "query_builder.add_edge(START, \"planner\")\n",
    "\n",
    "# Conditional edges from planner to multi_step or single_step\n",
    "query_builder.add_conditional_edges(\n",
    "    \"planner\",\n",
    "    lambda state: determine_planner_decision(state),\n",
    "    {\n",
    "        \"single_step\": END,\n",
    "        \"multi_step\": \"multi_step\"\n",
    "    }\n",
    ")\n",
    "\n",
    "query_builder.add_edge(\"multi_step\", END)\n",
    "\n",
    "def determine_planner_decision(state: QState):\n",
    "    planner_message = None\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if hasattr(msg, 'name') and msg.name == \"planner\":\n",
    "            planner_message = msg\n",
    "            break\n",
    "    \n",
    "    if planner_message:\n",
    "        planner_decision = planner_message.content.strip().lower()\n",
    "        \n",
    "        if \"single-step\" in planner_decision:\n",
    "            return \"single_step\"\n",
    "        elif \"multi-step\" in planner_decision:\n",
    "            return \"multi_step\"\n",
    "    \n",
    "    # Default fallback\n",
    "    return \"single_step\"\n",
    "\n",
    "query_agent_graph = query_builder.compile()\n",
    "\n",
    "## Create graph (Knowledge Agent)\n",
    "knowledge_builder = StateGraph(State)\n",
    "knowledge_builder.add_node(\"knowledge_supervisor\", knowledge_agent_supervisor_node)\n",
    "knowledge_builder.add_node(\"lmkg\", lmkg_node)\n",
    "knowledge_builder.add_node(\"hkg\", hkg_node)\n",
    "knowledge_builder.add_node(\"ds\", ds_node)\n",
    "knowledge_builder.add_node(\"primekg\", primekg_node)\n",
    "knowledge_builder.add_node(\"drugreviews\", drugreviews_node)\n",
    "knowledge_builder.add_node(\"wiki\", wiki_node)\n",
    "knowledge_builder.add_node(\"mayoclinic\", mayoclinic_node)\n",
    "knowledge_builder.add_node(\"llmself\", llmself_node)\n",
    "\n",
    "# Start with supervisor\n",
    "knowledge_builder.add_edge(START, \"knowledge_supervisor\")\n",
    "\n",
    "# Conditional edges FROM supervisor to workers and end\n",
    "knowledge_builder.add_conditional_edges(\n",
    "    \"knowledge_supervisor\",\n",
    "    lambda state: state[\"next\"],\n",
    "    {\n",
    "        \"lmkg\": \"lmkg\",\n",
    "        \"hkg\": \"hkg\",\n",
    "        \"primekg\": \"primekg\", \n",
    "        \"wiki\": \"wiki\",\n",
    "        \"mayoclinic\": \"mayoclinic\",\n",
    "        \"llmself\": \"llmself\",\n",
    "        \"ds\": \"ds\",\n",
    "        \"drugreviews\": \"drugreviews\",\n",
    "        \"FINISH\": END,\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Unconditional edges FROM workers back TO supervisor\n",
    "knowledge_builder.add_edge(\"lmkg\", \"knowledge_supervisor\")\n",
    "knowledge_builder.add_edge(\"hkg\", \"knowledge_supervisor\")\n",
    "knowledge_builder.add_edge(\"ds\", \"knowledge_supervisor\")\n",
    "knowledge_builder.add_edge(\"primekg\", \"knowledge_supervisor\")\n",
    "knowledge_builder.add_edge(\"drugreviews\", \"knowledge_supervisor\")\n",
    "knowledge_builder.add_edge(\"wiki\", \"knowledge_supervisor\")\n",
    "knowledge_builder.add_edge(\"mayoclinic\", \"knowledge_supervisor\")\n",
    "knowledge_builder.add_edge(\"llmself\", \"knowledge_supervisor\")\n",
    "\n",
    "knowledge_agent_graph = knowledge_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7949035",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAG Chain\n",
    "## Helper function (linker)\n",
    "def process_final_message(final_message):\n",
    "    stripped_message = final_message.strip()\n",
    "    \n",
    "    if '\\n' in stripped_message:\n",
    "        return [line.strip() for line in stripped_message.split('\\n') if line.strip()]\n",
    "    else:\n",
    "        return [stripped_message]\n",
    "\n",
    "## RAG Function\n",
    "def rag(test_query):\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(f\"[Test query]: {test_query}\")\n",
    "\n",
    "    # Query Agent\n",
    "    message_contents = []\n",
    "    for s in query_agent_graph.stream(\n",
    "        {\"messages\": [(\"user\", test_query)]},\n",
    "        {\"recursion_limit\": 100},\n",
    "    ):\n",
    "        \n",
    "        if 'messages' in str(s):\n",
    "            for key, value in s.items():\n",
    "                if isinstance(value, dict) and 'messages' in value:\n",
    "                    for msg in value['messages']:\n",
    "                        content = msg.content\n",
    "                        message_contents.append(content)\n",
    "\n",
    "    final_message = message_contents[-1]\n",
    "    message_list = process_final_message(final_message)\n",
    "\n",
    "    # Knowledge Agent\n",
    "    retrieved_references = {}\n",
    "\n",
    "    for msg in message_list:\n",
    "        final_state = None\n",
    "        \n",
    "        for s in knowledge_agent_graph.stream(\n",
    "            {\"messages\": [(\"user\", msg)]},\n",
    "            {\"recursion_limit\": 100}):\n",
    "            \n",
    "            for key, value in s.items():\n",
    "                if isinstance(value, dict):\n",
    "                    if 'references' in value:\n",
    "                        if final_state is None:\n",
    "                            final_state = {}\n",
    "                        final_state.update(value)\n",
    "\n",
    "        # Print final results\n",
    "        if final_state:\n",
    "            if 'references' in final_state:\n",
    "                retrieved_references[msg] = final_state['references']\n",
    "\n",
    "    print(f\"\\n[Retrieved reference]: {retrieved_references}\")\n",
    "    return retrieved_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07057be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## format_query\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../new_data/ddxplus_context.csv\")\n",
    "ques = df.loc[1]\n",
    "def format_query(ques):\n",
    "    question = ques['disease']\n",
    "    query = f\"What are the symptoms of '{question}'?\"\n",
    "    return query\n",
    "\n",
    "query = format_query(ques)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc9b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve contexts for 10 disease types\n",
    "import json\n",
    "\n",
    "df = pd.read_csv(\"../new_data/ddxplus_context.csv\")\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        query = format_query(df.loc[i])\n",
    "        if pd.notna(query):\n",
    "            result = rag(query)\n",
    "            result_str = json.dumps(result, ensure_ascii=False) # convert dict to string\n",
    "            df.loc[i, \"reference\"] = result_str\n",
    "            df.to_csv(\"../new_data/ddxplus_context.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at row {i}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeaf18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create knowledge base for 10 disease types (sources separated, filtered, no llmself)\n",
    "## FAISS vector store\n",
    "import pandas as pd\n",
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Step 1: Load CSV\n",
    "df = pd.read_csv(\"../new_data/ddxplus_context.csv\")\n",
    "\n",
    "documents = []\n",
    "\n",
    "# Step 2: Process each row\n",
    "for _, row in df.iterrows():\n",
    "    diagnosis = row['disease']\n",
    "    reference_json_str = row['reference_no_llm']\n",
    "    \n",
    "    try:\n",
    "        reference_dict = json.loads(reference_json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Failed to parse JSON for diagnosis: {diagnosis}\")\n",
    "        continue\n",
    "\n",
    "    # Step 3: Extract nested sources\n",
    "    for question, sources in reference_dict.items():\n",
    "        for source_name, content in sources.items():\n",
    "            if isinstance(content, str):\n",
    "                documents.append(Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\"diagnosis\": diagnosis, \"source\": source_name}\n",
    "                ))\n",
    "            elif isinstance(content, list):\n",
    "                for item in content:\n",
    "                    documents.append(Document(\n",
    "                        page_content=item,\n",
    "                        metadata={\"diagnosis\": diagnosis, \"source\": source_name}\n",
    "                    ))\n",
    "            elif isinstance(content, dict):\n",
    "                doc_text = \"\\n\".join([f\"{k}: {v}\" for k, v in content.items()])\n",
    "                documents.append(Document(\n",
    "                    page_content=doc_text,\n",
    "                    metadata={\"diagnosis\": diagnosis, \"source\": source_name}\n",
    "                ))\n",
    "            else:\n",
    "                print(f\"Unknown content type for {source_name} in {diagnosis}\")\n",
    "\n",
    "## Create knowledge base for 22 disease types\n",
    "# initiate faiss vector store and openai embedding\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "index = faiss.IndexFlatL2(len(OpenAIEmbeddings().embed_query(\" \")))\n",
    "vector_store = FAISS(\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents)\n",
    "save_path = \"./ddxplus_faiss\"\n",
    "vector_store.save_local(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b599bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load FAISS (sources separated, filtered, no llmself)\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# old ver: ddxplus_test_2_faiss\n",
    "vector_store = FAISS.load_local(\n",
    "    \"ddxplus_faiss\",\n",
    "    OpenAIEmbeddings(),\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "num_documents = len(vector_store.docstore._dict)\n",
    "print(f\"FAISS index loaded with {num_documents} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6527e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Ref + LLMSelf (gpt)\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "def reference_llmself(query: str, vector_store):\n",
    "    # 1 top similar ref from knowledge base + 1 llmself ans\n",
    "    options = \"Anemia, Boerhaave, Cluster headache, GERD, Influenza, Myocarditis, Panic attack, Pericarditis, Pneumonia, Sarcoidosis\"\n",
    "    # Step 1: Run vector similarity search using only the symptom query\n",
    "    results = vector_store.similarity_search(query, k=1)\n",
    "    if not results:\n",
    "        return {query: {\"llmself\": \"No relevant information found.\"}}\n",
    "\n",
    "    top_doc = results[0]\n",
    "    source = top_doc.metadata.get(\"source\", \"unknown\")\n",
    "    content = top_doc.page_content.strip()\n",
    "\n",
    "    # Step 2: Run LLM using query + options\n",
    "    llm_prompt = (\n",
    "        \"You are a Medical Diagnosis AI. Your task is to determine the most likely diagnosis based on the patient's symptom description.\\n\\n\"\n",
    "        f\"options: {options}\\n\\n\"\n",
    "        f\"Patient Description: {query}\\n\\n\"\n",
    "        \"Select the most appropriate diagnosis from the options provided, and briefly explain your reasoning based on the described symptoms.\\n\"\n",
    "        \"Be concise, medically accurate, and focus only on the symptoms mentioned.\"\n",
    "    )\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(f'[Test query]: {query}')\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": llm_prompt\n",
    "        }]\n",
    "    )\n",
    "    llm_result = response.choices[0].message.content.strip()\n",
    "\n",
    "    # Step 3: Combine and return\n",
    "    reference_dict = {\n",
    "        source: content,\n",
    "        \"llmself\": llm_result\n",
    "    }\n",
    "\n",
    "    return {query: reference_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9352585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ref + llmself (gpt)\n",
    "import json\n",
    "\n",
    "df = pd.read_csv(\"../new_data/ddxplus_400_result.csv\")\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        query = df.loc[i, \"EVIDENCES\"]\n",
    "        if pd.notna(query):\n",
    "            result = reference_llmself(query, vector_store)\n",
    "            result_str = json.dumps(result, ensure_ascii=False)\n",
    "            df.loc[i, \"gpt_reference\"] = result_str\n",
    "            df.to_csv(\"../new_data/ddxplus_400_result.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at row {i}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04c2b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Ref + LLMSelf (ds)\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize DeepSeek client\n",
    "os.environ[\"DEEPSEEK_API_KEY\"] = os.getenv('DEEPSEEK_API_KEY')\n",
    "os.environ[\"DEEPSEEK_BASE_URL\"] = os.getenv('DEEPSEEK_BASE_URL')\n",
    "client = OpenAI(api_key=os.environ[\"DEEPSEEK_API_KEY\"], base_url=os.environ[\"DEEPSEEK_BASE_URL\"])\n",
    "\n",
    "def reference_llmself(query: str, vector_store):\n",
    "    # 1 top similar ref from knowledge base + 1 llmself ans\n",
    "    options = \"Anemia, Boerhaave, Cluster headache, GERD, Influenza, Myocarditis, Panic attack, Pericarditis, Pneumonia, Sarcoidosis\"\n",
    "    # Step 1: Run vector similarity search using only the symptom query\n",
    "    results = vector_store.similarity_search(query, k=1)\n",
    "    if not results:\n",
    "        return {query: {\"llmself\": \"No relevant information found.\"}}\n",
    "\n",
    "    top_doc = results[0]\n",
    "    source = top_doc.metadata.get(\"source\", \"unknown\")\n",
    "    content = top_doc.page_content.strip()\n",
    "\n",
    "    # Step 2: Run LLM using query + options\n",
    "    llm_prompt = (\n",
    "        \"You are a Medical Diagnosis AI, answer with your internal knowledge.\\n\\n\"\n",
    "        \"Select the most suitable diagnosis based on the symptoms, and briefly explain your reasoning in max 3 sentences.\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        f\"Options: {options}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(f'[Test query]: {query}')\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": llm_prompt\n",
    "        }]\n",
    "    )\n",
    "    llm_result = response.choices[0].message.content.strip()\n",
    "\n",
    "    # Step 3: Combine and return\n",
    "    reference_dict = {\n",
    "        source: content,\n",
    "        \"llmself\": llm_result\n",
    "    }\n",
    "    \n",
    "    return {query: reference_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035858be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ref + llmself (ds)\n",
    "import json\n",
    "\n",
    "df = pd.read_csv(\"../new_data/ddxplus_400_result.csv\")\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        query = df.loc[i, \"EVIDENCES\"]\n",
    "        if pd.notna(query):\n",
    "            result = reference_llmself(query, vector_store)\n",
    "            result_str = json.dumps(result, ensure_ascii=False)\n",
    "            df.loc[i, \"ds_reference\"] = result_str\n",
    "            df.to_csv(\"../new_data/ddxplus_400_result.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at row {i}: {e}\")\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
